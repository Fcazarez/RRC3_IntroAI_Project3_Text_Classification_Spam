{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/blob/main/IntroAI_P3_0378585_Text_Classification_Spam_and_Ham_TF_IDF_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldwar1ImiNog"
      },
      "source": [
        "# **COMP-3703 (241551) Introduction to Artificial Intelligence**\n",
        "# **Project 3 - Text Classification-Spam and Ham and TF-IDF**\n",
        "# **Felix Cazarez**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ8u38hAdmqF"
      },
      "source": [
        "**Objective:**\n",
        "* Successfully implement a spam classifier to the YouTube comments from 5 different YouTube videos,\n",
        "* Create code to calculate TF-IDF scores for a list of sentences.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "From the UCI ML Repository, you will need the following files:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCp49F1u0ZIs"
      },
      "source": [
        "# Task 1: Data Exploration\n",
        "\n",
        "## 1.1 The Data\n",
        "* Read in the Psy file to your Jupyter Notebook.** (1 point)\n",
        "* Display the head and tail of the file.** (1 point)\n",
        "* Give the number of rows in the file.** (1 point)\n",
        "* Display the number of spam and ham comments for the file.** (2 points)\n",
        "* Discuss the balance of the dataset and the balance of the classes spam/ham. Based on this, what sort of classifiers are appropriate for the task?** (3 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOVehrql0ZIu"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfTransformer\n",
        "    )\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import (\n",
        "     train_test_split,\n",
        "     cross_val_score,\n",
        "     GridSearchCV\n",
        "     )\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6cr552x0ZIv",
        "outputId": "b1ed9d65-21fc-43d0-c609-235f65499903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head of the psy dataframe: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMMENT_ID</th>\n",
              "      <th>AUTHOR</th>\n",
              "      <th>DATE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
              "      <td>Julius NM</td>\n",
              "      <td>2013-11-07T06:20:48</td>\n",
              "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
              "      <td>adam riyati</td>\n",
              "      <td>2013-11-07T12:37:15</td>\n",
              "      <td>Hey guys check out my new channel and our firs...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
              "      <td>Evgeny Murashkin</td>\n",
              "      <td>2013-11-08T17:34:21</td>\n",
              "      <td>just for test I have to say murdev.com</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
              "      <td>ElNino Melendez</td>\n",
              "      <td>2013-11-09T08:28:43</td>\n",
              "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
              "      <td>GsMega</td>\n",
              "      <td>2013-11-10T16:05:38</td>\n",
              "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    COMMENT_ID            AUTHOR  \\\n",
              "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
              "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
              "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
              "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
              "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
              "\n",
              "                  DATE                                            CONTENT  \\\n",
              "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
              "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
              "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
              "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
              "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
              "\n",
              "   CLASS  \n",
              "0      1  \n",
              "1      1  \n",
              "2      1  \n",
              "3      1  \n",
              "4      1  "
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "git_url_psy = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube01-Psy.csv'\n",
        "df_psy = pd.read_csv (git_url_psy)\n",
        "print(\"Head of the psy dataframe: \")\n",
        "df_psy.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhm7N2u60ZIy",
        "outputId": "87f10df6-0f88-4128-f38c-6c87335de120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tail of the psy dataframe: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMMENT_ID</th>\n",
              "      <th>AUTHOR</th>\n",
              "      <th>DATE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
              "      <td>Carmen Racasanu</td>\n",
              "      <td>2014-11-14T13:27:52</td>\n",
              "      <td>How can this have 2 billion views when there's...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
              "      <td>diego mogrovejo</td>\n",
              "      <td>2014-11-14T13:28:08</td>\n",
              "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
              "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
              "      <td>2015-05-23T13:04:32</td>\n",
              "      <td>subscribe to me for call of duty vids and give...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
              "      <td>Photo Editor</td>\n",
              "      <td>2015-06-05T14:14:48</td>\n",
              "      <td>hi guys please my android photo editor downloa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
              "      <td>Ray Benich</td>\n",
              "      <td>2015-06-05T18:05:16</td>\n",
              "      <td>The first billion viewed this because they tho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                COMMENT_ID  \\\n",
              "345      z13th1q4yzihf1bll23qxzpjeujterydj   \n",
              "346  z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
              "347    z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
              "348    z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
              "349  z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
              "\n",
              "                                   AUTHOR                 DATE  \\\n",
              "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
              "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
              "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
              "348                          Photo Editor  2015-06-05T14:14:48   \n",
              "349                            Ray Benich  2015-06-05T18:05:16   \n",
              "\n",
              "                                               CONTENT  CLASS  \n",
              "345  How can this have 2 billion views when there's...      0  \n",
              "346         I don't now why I'm watching this in 2014﻿      0  \n",
              "347  subscribe to me for call of duty vids and give...      1  \n",
              "348  hi guys please my android photo editor downloa...      1  \n",
              "349  The first billion viewed this because they tho...      0  "
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Tail of the psy dataframe: \")\n",
        "df_psy.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8J8UZzg0ZIz",
        "outputId": "5efea0cb-0578-47d4-bac4-232aafb2d03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows psy dataframe: 350\n"
          ]
        }
      ],
      "source": [
        "num_rows = df_psy.shape[0]\n",
        "print(\"Number of rows psy dataframe:\", num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90zjLpOf0ZI3",
        "outputId": "50ccb245-c9c0-4260-8e68-302ae9954df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Spam Comments: 175\n",
            "Number of Ham Comments: 175\n"
          ]
        }
      ],
      "source": [
        "# Count the number of spam and ham comments\n",
        "spam_count = df_psy[df_psy['CLASS'] == 1].shape[0]\n",
        "ham_count = df_psy[df_psy['CLASS'] == 0].shape[0]\n",
        "\n",
        "# Display the counts\n",
        "print(f\"Number of Spam Comments: {spam_count}\")\n",
        "print(f\"Number of Ham Comments: {ham_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYITrNqn0ZI4"
      },
      "source": [
        "**Comments:**\n",
        "\n",
        "Ham and spam are perfectly balanced, we can say the data is good to go for modelling since the training stage will not have bias and the chosen model can be less complex. The classifiers suitable for this balanced dataset include:\n",
        "\n",
        "* Logistic Regression: This classifier is a good starting point and tends to work well for balanced datasets.\n",
        "\n",
        "* Decision Trees: Decision trees can handle balanced datasets effectively and are capable of capturing complex relationships in the data.\n",
        "\n",
        "* Random Forests: This ensemble method, which consists of multiple decision trees, can provide robust performance and handle both numerical and categorical features well.\n",
        "\n",
        "* Support Vector Machines (SVM): SVMs can work effectively for balanced datasets and are capable of handling high-dimensional feature spaces.\n",
        "\n",
        "* K-Nearest Neighbors (KNN): KNN is a simple and effective algorithm for classification, and it tends to work well when the dataset is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfsUlLei0ZI5"
      },
      "source": [
        "## 1.2 Data Transformation\n",
        "* Create a bag-of-words for the Psy file. The fit and transform should be done on the ‘CONTENT’ of your data.** (2 points)\n",
        "* Display the matrix information. For example.** (2 points)\n",
        "* How many different words are in your bag of words?** (1 point)\n",
        "* Display the 349th comment in the Psy file.** (2 points)\n",
        "* Using ’analyze’ ,or something similar, give the breakdown of the 349th comment.** (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XUvYF1c0ZJS",
        "outputId": "6ea3d9a3-a18c-4fae-e4d5-3061d7a83e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag-of-Words Matrix Information:\n",
            "Shape: (350, 1418)\n",
            "Number of different words: 1418\n",
            "============================================================\n",
            "\n",
            "349th Comment:\n",
            "hi guys please my android photo editor download. thanks https://play.google.com/store/apps/details?id=com.butalabs.photo.editor﻿\n",
            "\n",
            "Breakdown of 349th Comment: ['hi', 'guys', 'please', 'my', 'android', 'photo', 'editor', 'download', 'thanks', 'https', 'play', 'google', 'com', 'store', 'apps', 'details', 'id', 'com', 'butalabs', 'photo', 'editor']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the CountVectorizer\n",
        "content_data = df_psy['CONTENT'].tolist()\n",
        "\n",
        "# Create the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the 'CONTENT' data into a bag-of-words matrix\n",
        "bow_matrix = vectorizer.fit_transform(content_data)\n",
        "\n",
        "# Display the matrix information\n",
        "print(\"Bag-of-Words Matrix Information:\")\n",
        "print(f\"Shape: {bow_matrix.shape}\")\n",
        "print(f\"Number of different words: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Display the 349th comment in the Psy file\n",
        "comment_349 = content_data[348]  # Indexing starts from 0\n",
        "print(\"==\"*30)\n",
        "print(f\"\\n349th Comment:\\n{comment_349}\")\n",
        "\n",
        "# Using 'analyze' or similar, give the breakdown of the 349th comment\n",
        "analyze_result = vectorizer.build_analyzer()(comment_349)\n",
        "print(f\"\\nBreakdown of 349th Comment: {analyze_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwBtMb6I0ZJT"
      },
      "source": [
        "# 2. Training and Testing Sets\n",
        "* Shuffle your dataset(frac=1).** (1 point)\n",
        "* Create your training and testing splits by using the first 300 entries for training and the remaining for testing. Name them appropriately.** (3 points)\n",
        "* Create your training and testing attributes BOW. Name them appropriately.** (2 points)\n",
        "* Create your training and testing labels. Name them appropriately.** (2 points)\n",
        "* Output the matrix information of d_train_att and d_test_att. What are the dimensions of the matrices?** (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBUiinzK0ZJT",
        "outputId": "676cda14-39ab-47b3-e374-fae53e8faf00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(350, 1418)"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow_matrix.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmLNXZXd0ZJX",
        "outputId": "181c5180-94f4-4c4c-c994-e96b9ee36745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train (Features): (300,)\n",
            "Shape of X_test (Features): (50,)\n",
            "Shape of y_train (Labels): (300,)\n",
            "Shape of y_test (Labels): (50,)\n",
            "Matrix Information for Training Data (d_train_att):\n",
            "Shape: (300, 1262)\n",
            "\n",
            "Matrix Information for Testing Data (d_test_att):\n",
            "Shape: (50, 1262)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Shuffle your dataset\n",
        "df_psy_shuffled = df_psy.sample(frac=1, random_state=42)\n",
        "\n",
        "X = df_psy_shuffled['CONTENT']\n",
        "y = df_psy_shuffled['CLASS']\n",
        "\n",
        "# Create training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,                          # Features\n",
        "    y,                          # Labels\n",
        "    test_size=50,               # Adjust the test size as needed\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#verifying shapes\n",
        "print(\"Shape of X_train (Features):\", X_train.shape)\n",
        "print(\"Shape of X_test (Features):\", X_test.shape)\n",
        "print(\"Shape of y_train (Labels):\", y_train.shape)\n",
        "print(\"Shape of y_test (Labels):\", y_test.shape)\n",
        "\n",
        "# Create bag-of-words for training and testing data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "#X_train_bow = vectorizer.transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# Output matrix information\n",
        "print(\"Matrix Information for Training Data (d_train_att):\")\n",
        "print(f\"Shape: {X_train_bow.shape}\")\n",
        "\n",
        "print(\"\\nMatrix Information for Testing Data (d_test_att):\")\n",
        "print(f\"Shape: {X_test_bow.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm540hEV0ZJZ"
      },
      "source": [
        "# 3. Random Forest Classifier\n",
        "* Implement a Random Forest classifier. With 50 trees, output.** (3 points)\n",
        "* Train the classifier on the training data and test its performance on the testing data.** (2 points)\n",
        "* Print the training and testing accuracies.** (2 points)\n",
        "* Cross validate using 3 folds. Output the accuracies of the folds.** (3 points)\n",
        "* Generate a confusion matrix for the Random Forest classifier's predictions on the test data.** (3 points)\n",
        "* Visualize this matrix (you may need to implement or use a utility function for visualization).** (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrrOAIQS0ZJa",
        "outputId": "1f8ace12-3ee7-43d6-f287-a3c145dcf6f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.0\n",
            "Testing Accuracy: 0.9\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Implement a Random Forest classifier with 50 trees\n",
        "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "# Test the classifier on the testing data\n",
        "y_pred_test = rf_classifier.predict(X_test_bow)\n",
        "\n",
        "# Print the training and testing accuracies\n",
        "#train_accuracy = accuracy_score(y_train, rf_classifier.predict(X_train_bow))\n",
        "#test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# training accuracy\n",
        "train_accuracy = rf_classifier.score(X_train_bow, y_train)\n",
        "# test accuracy\n",
        "test_accuracy = rf_classifier.score(X_test_bow, y_test)\n",
        "\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRQNrbbx0ZJb",
        "outputId": "e58cc79e-d8bd-48b9-e48b-17d8fb761a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracies: [0.93 0.98 0.97]\n",
            "------------------------Cross-Validation Accuracies with 3 Folds:\n",
            "Fold 1: 0.9300\n",
            "Fold 2: 0.9800\n",
            "Fold 3: 0.9700\n"
          ]
        }
      ],
      "source": [
        "# Cross validate using 3 folds and output the accuracies of the folds\n",
        "cross_val_accuracies = cross_val_score(rf_classifier, X_train_bow, y_train, cv=3)\n",
        "print(\"Cross-Validation Accuracies:\", cross_val_accuracies)\n",
        "print(\"------------------------Cross-Validation Accuracies with 3 Folds:\")\n",
        "\n",
        "#itatate to show the three folds\n",
        "for fold, accuracy in enumerate(cross_val_accuracies, 1):\n",
        "    print(f\"Fold {fold}: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrElrsRp0ZJc",
        "outputId": "6ab4adb6-7f43-420b-c3fd-fc0371143f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[27  0]\n",
            " [ 5 18]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate a confusion matrix for the Random Forest classifier's predictions on the test data\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i22aCy_I0ZJd"
      },
      "source": [
        "# 4. Pipeline Data\n",
        "* Concatenate your 5 files into one using .concat.** (3 points)\n",
        "* Provided you have concatenated the files correctly the length of your new file should be close to 2000. Output the length.** (1 point)\n",
        "* Check the spam-to-ham ratio in the new file; they should be balanced- about 1000 each. Output the number of spams and the number of hams.** (2 points)\n",
        "* Shuffle the new data and create content and label sets, and name them appropriately.** (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASAZlzWY0ZJd"
      },
      "outputs": [],
      "source": [
        "git_url_psy = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube01-Psy.csv'\n",
        "df_psy = pd.read_csv (git_url_psy)\n",
        "git_url_kate = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube02-KatyPerry.csv'\n",
        "df_kate = pd.read_csv (git_url_kate)\n",
        "git_url_lmfao = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube03-LMFAO.csv'\n",
        "df_lmfao = pd.read_csv (git_url_lmfao)\n",
        "git_url_eminem = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube04-Eminem.csv'\n",
        "df_eminem = pd.read_csv (git_url_eminem)\n",
        "git_url_shakira = 'https://raw.githubusercontent.com/Fcazarez/RRC3_IntroAI_Project3_Text_Classification_Spam/main/Youtube05-Shakira.csv'\n",
        "df_shakira = pd.read_csv (git_url_shakira)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHCTDHx00ZJd",
        "outputId": "8d23284d-1c55-455f-8371-44446669a34c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(350, 350, 438, 448, 370)"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_psy), len(df_kate), len(df_lmfao), len(df_eminem), len(df_sharika)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dI4LfnM0ZJe",
        "outputId": "aa7b2d29-22b3-49ca-dd0f-300cc5e883a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1956 entries, 0 to 1955\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   COMMENT_ID  1956 non-null   object\n",
            " 1   AUTHOR      1956 non-null   object\n",
            " 2   DATE        1711 non-null   object\n",
            " 3   CONTENT     1956 non-null   object\n",
            " 4   CLASS       1956 non-null   int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 76.5+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of all DataFrames\n",
        "all_dataframes = [df_psy, df_kate, df_lmfao, df_eminem, df_shakira]\n",
        "\n",
        "# Concatenate into one DataFrame\n",
        "concat_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "concat_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVD3_dUN0ZJe",
        "outputId": "25f8a7f7-e35e-418c-ebf7-83087bbd425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the concatenated file: 1956\n"
          ]
        }
      ],
      "source": [
        "len_concat = len(concat_df)\n",
        "\n",
        "print(\"Length of the concatenated file:\", len_concat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiV3WhH80ZJf",
        "outputId": "8a7b73dd-5f96-4b05-bac9-0f699232b71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Spam comments in concat df: 1005\n",
            "Number of Ham comments in concat df: 951\n",
            "Spam-to-Ham Ratio: 1.0567823343848581\n",
            "Spam Proportion: 51.38%\n"
          ]
        }
      ],
      "source": [
        "spam_count = concat_df['CLASS'].value_counts()[1]\n",
        "ham_count = concat_df['CLASS'].value_counts()[0]\n",
        "total_comments = len(concat_df)\n",
        "\n",
        "print(\"Number of Spam comments in concat df:\", spam_count)\n",
        "print(\"Number of Ham comments in concat df:\", ham_count)\n",
        "\n",
        "# Calculate and print the spam-to-ham ratio\n",
        "spam_to_ham_ratio = spam_count / ham_count\n",
        "print(\"Spam-to-Ham Ratio:\", spam_to_ham_ratio)\n",
        "\n",
        "# Calculate and print the spam proportion as a percentage with 2 decimal places\n",
        "spam_proportion = (spam_count / total_comments) * 100\n",
        "print(\"Spam Proportion:\", \"{:.2f}%\".format(spam_proportion))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxmp0GPJ0ZJf",
        "outputId": "ed02fd95-26bc-4d5b-b320-1897187c9398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of content_set: (1956,)\n",
            "Shape of label_set: (1956,)\n"
          ]
        }
      ],
      "source": [
        "# Shuffle the concatenated DataFrame\n",
        "shuffled_df = shuffle(concat_df, random_state=42)\n",
        "\n",
        "# Create content and label sets\n",
        "X_features = shuffled_df['CONTENT']\n",
        "y_labels = shuffled_df['CLASS']\n",
        "\n",
        "# Verify shapes\n",
        "print(\"Shape of content_set:\", X_features.shape)\n",
        "print(\"Shape of label_set:\", y_labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEwrm7r70ZJf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKlCO_0w0ZJg"
      },
      "source": [
        "# 5. Pipeline Creation\n",
        "* Read the sci-kit learn documentation on Pipelines. Name 3 advantages to using a pipeline. You can use any reference you wish to answer this question, just be sure to include the reference in your response.** (5 points)\n",
        "* Create a two-step pipeline with a bag-of-words step and a random forest step.** (3 points)\n",
        "* Output the pipeline to display its steps.** (1 point)\n",
        "* Fit your pipeline with the first 1500 entries of the content and labels. Output.** (3 points)\n",
        "* Use .score to score your pipeline.** (2 points)\n",
        "* Use your pipeline to predict whether the following two comments are spam or ham.** (5 points)\n",
        "   1. \"what a neat video\"\n",
        "   2. \"plz subscribe to my channel\"\n",
        "* Cross-validate using your pipeline. Use cv=3. Print out the accuracies.** (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcYjVohN0ZJg"
      },
      "source": [
        "## 🚀 Advantages of Using Machine Learning Pipelines\n",
        "\n",
        "1. **Convenience and Simplicity:** Pipelines streamline the complexity of machine learning workflows, consolidating preprocessing, feature engineering, and model training into a single, cohesive structure. This enhances code readability and maintainability, offering a convenient way to manage intricate tasks and evolving models (Müller and Guido, 2016).\n",
        "\n",
        "2. **Prevents Data Leakage:** Data leakage, the bane of model evaluation, is mitigated by pipelines. They ensure consistent application of preprocessing steps to both training and testing data, promoting fair model assessments. Carefully sequencing preprocessing within the pipeline is key to preventing unintentional information flow during training (Pedregosa et al., 2011).\n",
        "\n",
        "3. **Ease of Deployment and Reproducibility:** Pipelines facilitate seamless deployment and reproducibility, encapsulating the entire workflow from data preprocessing to model training. This enables easy serialization and deployment, ensuring production environments mirror development accurately. By embracing pipelines, achieve reproducibility in deployment and simplify collaboration through a standardized representation of the machine learning process (Géron, 2019).\n",
        "\n",
        "4. **Efficiency:** A machine learning pipeline is an automated sequence of processes that enables data to flow from its raw state to one that is refined and valuable for machine learning models¹. This brings order and efficiency to what could otherwise be a chaotic process¹.\n",
        "\n",
        "5. **Consistency:** Pipelines ensure that the same sequence of transformations is applied to the data every time, which is crucial for the reproducibility of experiments¹. This consistency is particularly important when using machine learning to make predictions or classifications based on new data¹.\n",
        "\n",
        "6. **Scalability:** Machine learning pipelines are iterative as every step is repeated to continuously improve the accuracy of the model and achieve a successful algorithm². This makes the model scalable, as it can handle larger datasets and more complex transformations².\n",
        "\n",
        "7. **Reproducibility and Collaboration:** A well-constructed machine learning pipeline offers a host of benefits, making it a powerful tool for any data scientist or AI engineer¹. Its core advantages are tied to efficiency, consistency, scalability, reproducibility, and collaboration¹.\n",
        "\n",
        "8. **Control:** The main objective of having a proper pipeline for any ML model is to exercise control over it. A well-organised pipeline makes the implementation more flexible².\n",
        "\n",
        "9. **Real-time Processing and Increased Privacy:** ML pipeline ensures smooth functioning of ML inference at edge level devices where the data generation plays a crucial part and offers features like lower cost, real-time processing, and increased privacy³.\n",
        "\n",
        "🌐 In summary, machine learning pipelines offer a powerful toolkit to organize code, prevent common pitfalls, and simplify deployment.\n",
        "\n",
        "**References:**\n",
        "- (1) [Machine Learning Pipelines: Benefits, Challenges, Use Cases](https://plat.ai/blog/machine-learning-pipeline/).\n",
        "- (2) [What is a Pipeline in Machine Learning? How to create one?](https://medium.com/analytics-vidhya/what-is-a-pipeline-in-machine-learning-how-to-create-one-bda91d0ceaca).\n",
        "- (3) [An overview of Machine Learning pipeline and its importance](https://www.design-reuse.com/articles/53595/an-overview-of-machine-learning-pipeline-and-its-importance.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWfDkAaM0ZJh",
        "outputId": "36561672-e142-49ff-e1a1-678474426d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steps for the pipeline: [('bow', CountVectorizer()), ('rf', RandomForestClassifier())]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the steps for the pipeline\n",
        "steps = [\n",
        "    ('bow', CountVectorizer()),        # Step 1: Bag-of-Words\n",
        "    ('rf', RandomForestClassifier())   # Step 2: Random Forest Classifier\n",
        "]\n",
        "\n",
        "print(\"Steps for the pipeline:\", steps)\n",
        "\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Now you can use the pipeline for fitting and predicting\n",
        "# For example, to fit the pipeline on your training data:\n",
        "# pipeline.fit(X_train, y_train)\n",
        "\n",
        "# And to make predictions on new data:\n",
        "# predictions = pipeline.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACsT9eA0ZJh",
        "outputId": "ac931525-d707-4355-ff7b-abf7453fdbca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train_subset: (1500,)\n",
            "Shape of y_train_subset: (1500,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('bow', CountVectorizer()), ('rf', RandomForestClassifier())])"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming you have X_train and y_train from your data\n",
        "X_features_subset = X_features[:1500]\n",
        "y_labels_subset = y_labels[:1500]\n",
        "print(\"Shape of X_train_subset:\", X_features_subset.shape)\n",
        "print(\"Shape of y_train_subset:\", y_labels_subset.shape)\n",
        "\n",
        "# Fit the pipeline with the first 1500 entries\n",
        "pipeline.fit(X_features_subset, y_labels_subset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfxcyIOW0ZJi",
        "outputId": "c9c9fc87-fe6b-4101-864c-dc5ad54a6ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have X_test and y_test from your data\n",
        "score = pipeline.score(X_test, y_test)\n",
        "\n",
        "# Print the accuracy score\n",
        "print(\"Accuracy Score:\", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "631xEcnw0ZJj",
        "outputId": "1787d609-5117-4cf3-8cb5-abab4fba9c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comment: 'what a neat video' - Predicted Class: Ham\n",
            "Comment: 'plz subscribe to my channel' - Predicted Class: Spam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Predicted Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what a neat video</td>\n",
              "      <td>Ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plz subscribe to my channel</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Comment Predicted Class\n",
              "0            what a neat video             Ham\n",
              "1  plz subscribe to my channel            Spam"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# List new_comments\n",
        "new_comments = [\n",
        "    \"what a neat video\",\n",
        "    \"plz subscribe to my channel\"\n",
        "    ]\n",
        "\n",
        "# Use the pipeline to predict\n",
        "predictions = pipeline.predict(new_comments)\n",
        "\n",
        "# Print the predictions\n",
        "for comment, prediction in zip(new_comments, predictions):\n",
        "    print(f\"Comment: '{comment}' - Predicted Class: {'Spam' if prediction == 1 else 'Ham'}\")\n",
        "\n",
        "# Create a DataFrame\n",
        "df_predictions = pd.DataFrame({\n",
        "    'Comment': new_comments,\n",
        "    'Predicted Class': [ 'Spam' if prediction == 1 else 'Ham' for prediction in predictions]\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "df_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l82ir-__0ZJk",
        "outputId": "1b5f89d4-cbd2-44ce-8f83-4bdf7a4a78af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.9400\n",
            "Fold 2: Accuracy = 0.9500\n",
            "Fold 3: Accuracy = 0.9580\n",
            "\n",
            "Average Accuracy: 0.9493\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation with the pipeline\n",
        "cv_scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X_features_subset,\n",
        "    y_labels_subset,\n",
        "    cv=3,\n",
        "    scoring=\"accuracy\"\n",
        "    )\n",
        "\n",
        "# Print out the accuracies\n",
        "for fold, accuracy in enumerate(cv_scores, 1):\n",
        "    print(f\"Fold {fold}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"\\nAverage Accuracy: {cv_scores.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXqyO4ot0ZJk"
      },
      "source": [
        "# 6. Pipeline 2 Creation\n",
        "* Create a second pipeline named pipeline2 which includes a TfidfTransformer step.** (3 points)\n",
        "* Cross validate pipeline2 with 3 folds. Output the accuracy.** (3 points)\n",
        "* Use the following parameter dictionary to perform a grid search.** (3 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E05vCj2L0ZJm"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTPA3C0U0ZJo"
      },
      "source": [
        "* Perform the grid search, you can reduce n_estimators to 2 values and max_features to 1000 and 2000 to speed things up.** (3 points)\n",
        "* Print out the best parameter settings.** (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANhF36eA0ZJp"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Define the steps for the second pipeline\n",
        "steps_pipeline2 = [\n",
        "    ('bow', CountVectorizer()),           # Step 1: Bag-of-Words\n",
        "    ('tfidf', TfidfTransformer()),        # Step 2: TF-IDF Transformation\n",
        "    ('rf', RandomForestClassifier())      # Step 3: Random Forest Classifier\n",
        "]\n",
        "\n",
        "# Create the second pipeline (pipeline2)\n",
        "pipeline2 = Pipeline(steps_pipeline2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OqoUbvS0ZJq",
        "outputId": "ab94b0d2-4686-4a22-ed29-914c60776f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.9400\n",
            "Fold 2: Accuracy = 0.9460\n",
            "Fold 3: Accuracy = 0.9520\n",
            "\n",
            "Average Accuracy: 0.9460\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fold</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fold  Accuracy\n",
              "0     1     0.940\n",
              "1     2     0.946\n",
              "2     3     0.952"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the number of folds\n",
        "num_folds = 3\n",
        "\n",
        "# Perform cross-validation with pipeline2\n",
        "cv_scores_pipeline2 = cross_val_score(\n",
        "    pipeline2,\n",
        "    X_features_subset,\n",
        "    y_labels_subset,\n",
        "    cv=num_folds,\n",
        "    scoring='accuracy'\n",
        "    )\n",
        "\n",
        "# Print out the accuracies\n",
        "for fold, accuracy in enumerate(cv_scores_pipeline2, 1):\n",
        "    print(f\"Fold {fold}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"\\nAverage Accuracy: {cv_scores_pipeline2.mean():.4f}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for the accuracies of pipeline2\n",
        "df_accuracies_pipeline2 = pd.DataFrame({\n",
        "    'Fold': range(1, num_folds + 1),\n",
        "    'Accuracy': cv_scores_pipeline2\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "df_accuracies_pipeline2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTGXFvfa0ZJr",
        "outputId": "146472a6-eb18-402b-909e-8233d88ba807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'bow__max_features': 2000, 'bow__ngram_range': (1, 1), 'bow__stop_words': None, 'rf__n_estimators': 50, 'tfidf__use_idf': True}\n",
            "Best Score: 0.9599665978976324\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter dictionary\n",
        "parameters = {\n",
        "    'bow__max_features': [None, 1000, 2000],  # 'bow' is the name of CountVectorizer step\n",
        "    'bow__ngram_range': [(1, 1), (1, 2)],\n",
        "    'bow__stop_words': ['english', None],\n",
        "    'tfidf__use_idf': [True, False],  # 'tfidf' is the name of TfidfTransformer step\n",
        "    'rf__n_estimators': [20, 50]  # 'rf' is the name of RandomForestClassifier step\n",
        "}\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline2,\n",
        "    parameters,\n",
        "    cv=3,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# get best params and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best parameter settings\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyLASCsr0ZJ7",
        "outputId": "f8bcb2d3-c9cc-4caf-dd85-26a10ff826a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Score: 95.9967%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Parameter</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bow__max_features</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bow__ngram_range</td>\n",
              "      <td>(1, 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bow__stop_words</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rf__n_estimators</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tfidf__use_idf</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Parameter   Value\n",
              "0  bow__max_features    2000\n",
              "1   bow__ngram_range  (1, 1)\n",
              "2    bow__stop_words    None\n",
              "3   rf__n_estimators      50\n",
              "4     tfidf__use_idf    True"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df_best_params = pd.DataFrame(list(best_params.items()), columns=['Parameter', 'Value'])\n",
        "\n",
        "\n",
        "print(\"Best Score: {:.4%}\".format(best_score))\n",
        "# Display the DataFrame\n",
        "df_best_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmFlP1mv0ZJ8"
      },
      "source": [
        "# 7. TF Calculation\n",
        "Write python code that takes as an input a list of sentences and outputs the TF for every term in every sentence. (7 points)\n",
        "\n",
        "**Input:** list of sentences\n",
        "\n",
        "**Output:** the term frequency for every word in every sentence\n",
        "\n",
        "**Test Cases:**\n",
        "```python\n",
        "sentences = [\n",
        "    \"Python is a great programming language\",\n",
        "    \"Python can be used for a wide variety of programming tasks\",\n",
        "    \"It's easy to learn Python\"\n",
        "]\n",
        "sentences = [\"I love math\", \"Math is great\", \"Math rules\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq0K2u790ZJ9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    # Tokenize the sentence and apply regex to split and remove punctuation\n",
        "    words = re.findall(r'\\w+', sentence.lower())\n",
        "    return words\n",
        "\n",
        "def calculate_tf(sentences):\n",
        "    term_frequency = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize each sentence and calculate term frequency\n",
        "        words = tokenize_sentence(sentence)\n",
        "        tf = Counter(words)\n",
        "        term_frequency.append(tf)\n",
        "    return term_frequency\n",
        "\n",
        "def output_term_frequency(sentences, set_name):\n",
        "    term_frequency = calculate_tf(sentences)\n",
        "    print(f\"{set_name}:\")\n",
        "\n",
        "    for i, tf in enumerate(term_frequency, 1):\n",
        "        # Display term frequency for each sentence in the set\n",
        "        print(f\"================ Term Frequency for Sentence {i}: =================\")\n",
        "        for term, frequency in tf.items():\n",
        "            print(f\"{term}: {frequency}\")\n",
        "        print()\n",
        "\n",
        "def create_dataframe(sentences, set_name):\n",
        "    term_frequency_list = calculate_tf(sentences)\n",
        "    data = []\n",
        "\n",
        "    for i, tf in enumerate(term_frequency_list, 1):\n",
        "        # Create a list of dictionaries for each sentence\n",
        "        sentence_data = [{\"Term\": term, \"Frequency\": frequency, \"Sentence\": f\"Sentence {i}\"}\n",
        "                         for term, frequency in tf.items()]\n",
        "        data.extend(sentence_data)\n",
        "\n",
        "    # Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIfMG-Mg0ZJ-",
        "outputId": "a8387b67-58fc-452f-e957-9b5669e143a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences 1:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Python is a great programming language',\n",
              " 'Python can be used for a wide variety of programming tasks',\n",
              " \"It's easy to learn Python\"]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print (\"Sentences 1:\")\n",
        "sentences_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQjAGCrJ0ZJ-",
        "outputId": "cb4d1761-7a54-4f96-f80e-c12225910a03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Term</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>python</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>great</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>programming</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>language</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>python</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>can</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>be</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>used</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>wide</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>variety</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>of</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>programming</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tasks</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>it</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>s</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>easy</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>to</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>learn</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>python</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Term  Frequency    Sentence\n",
              "0        python          1  Sentence 1\n",
              "1            is          1  Sentence 1\n",
              "2             a          1  Sentence 1\n",
              "3         great          1  Sentence 1\n",
              "4   programming          1  Sentence 1\n",
              "5      language          1  Sentence 1\n",
              "6        python          1  Sentence 2\n",
              "7           can          1  Sentence 2\n",
              "8            be          1  Sentence 2\n",
              "9          used          1  Sentence 2\n",
              "10          for          1  Sentence 2\n",
              "11            a          1  Sentence 2\n",
              "12         wide          1  Sentence 2\n",
              "13      variety          1  Sentence 2\n",
              "14           of          1  Sentence 2\n",
              "15  programming          1  Sentence 2\n",
              "16        tasks          1  Sentence 2\n",
              "17           it          1  Sentence 3\n",
              "18            s          1  Sentence 3\n",
              "19         easy          1  Sentence 3\n",
              "20           to          1  Sentence 3\n",
              "21        learn          1  Sentence 3\n",
              "22       python          1  Sentence 3"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_set_1 = create_dataframe(sentences_1, \"Sentences Set 1\")\n",
        "df_set_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZDv8bze0ZJ_",
        "outputId": "6bb0921d-0f97-4d91-c75d-06c673f4c8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences 2:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I love math', 'Math is great', ' Math rules']"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print (\"Sentences 2:\")\n",
        "sentences_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXMlhy_J0ZKA",
        "outputId": "458f3f2c-0b6a-42ad-d9fd-f4f162f310c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Term</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>math</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>math</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>great</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>math</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rules</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentence 3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Term  Frequency    Sentence\n",
              "0      i          1  Sentence 1\n",
              "1   love          1  Sentence 1\n",
              "2   math          1  Sentence 1\n",
              "3   math          1  Sentence 2\n",
              "4     is          1  Sentence 2\n",
              "5  great          1  Sentence 2\n",
              "6   math          1  Sentence 3\n",
              "7  rules          1  Sentence 3"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_set_2 = create_dataframe(sentences_2, \"Sentences Set 2\")\n",
        "df_set_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tasios9n0ZKA"
      },
      "source": [
        "# 8. IDF Calculation\n",
        "\n",
        "Write Python code that calculates the inverse document frequency for a list of sentences.\n",
        "\n",
        "## Inputs\n",
        "- List of sentences\n",
        "\n",
        "## Outputs\n",
        "- Inverse document frequency for each word in the list of sentences.\n",
        "\n",
        "## Test Cases\n",
        "```python\n",
        "sentences_1 = [\n",
        "    \"Python is a great programming language\",\n",
        "    \"Python can be used for a wide variety of programming tasks\",\n",
        "    \"It's easy to learn Python\"\n",
        "]\n",
        "\n",
        "sentences_2 = [\n",
        "    \"I love math\",\n",
        "    \"Math is great\",\n",
        "    \" Math rules\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QTQlPvH0ZKB"
      },
      "outputs": [],
      "source": [
        "# Test Cases\n",
        "sentences_1 = [\n",
        "    \"Python is a great programming language\",\n",
        "    \"Python can be used for a wide variety of programming tasks\",\n",
        "    \"It's easy to learn Python\"\n",
        "]\n",
        "\n",
        "sentences_2 = [\n",
        "    \"I love math\",\n",
        "    \"Math is great\",\n",
        "    \" Math rules\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLWN73q60ZKC"
      },
      "outputs": [],
      "source": [
        "def calculate_idf(sentences):\n",
        "    words_in_sentences = set()\n",
        "    for sentence in sentences:\n",
        "        words = tokenize_sentence(sentence)\n",
        "        words_in_sentences.update(words)\n",
        "\n",
        "    idf = {}\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    for word in words_in_sentences:\n",
        "        sentences_containing_word = sum(1 for sentence in sentences if word in tokenize_sentence(sentence))\n",
        "        idf[word] = math.log(total_sentences / (sentences_containing_word + 1))\n",
        "\n",
        "    return idf\n",
        "\n",
        "def output_idf(sentences, set_name):\n",
        "    idf = calculate_idf(sentences)\n",
        "\n",
        "    print(f\"{set_name}:\")\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        print(f\"--------------------IDF for Sentence {i}:\")\n",
        "        words = tokenize_sentence(sentence)\n",
        "        for word in words:\n",
        "            print(f\"'{word}': {idf[word]}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "# calculate and print frequency for set_1\n",
        "output_idf(sentences_1, \"Sentences Set 1\")\n",
        "df_set_1 = create_dataframe(sentences_1, \"Sentences Set 1\")\n",
        "df_set_1\n",
        "\n",
        "# calculate and print frequency for set_2\n",
        "output_idf(sentences_2, \"Sentences Set 2\")\n",
        "df_set_2 = create_dataframe(sentences_2, \"Sentences Set 2\")\n",
        "df_set_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKMhpV1o0ZKD"
      },
      "outputs": [],
      "source": [
        "def create_dataframe(sentences, set_name):\n",
        "    term_frequency_list = calculate_tf(sentences)\n",
        "    data = []\n",
        "\n",
        "    for i, tf in enumerate(term_frequency_list, 1):\n",
        "        # Create a list of dictionaries for each sentence\n",
        "        sentence_data = [{\"Term\": term, \"Frequency\": frequency, \"Sentence\": f\"Sentence {i}\"}\n",
        "                         for term, frequency in tf.items()]\n",
        "        data.extend(sentence_data)\n",
        "\n",
        "    # Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4MNAy7E0ZKE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_idf(sentences):\n",
        "    words_in_sentences = set()\n",
        "    sentence_word_count = {}\n",
        "\n",
        "    # Collect unique words and count their occurrences in each sentence\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        words = tokenize_sentence(sentence)\n",
        "        words_in_sentences.update(words)\n",
        "        sentence_word_count[i] = Counter(words)\n",
        "\n",
        "    idf = {}\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    # Calculate IDF scores for each word\n",
        "    for word in words_in_sentences:\n",
        "        sentences_containing_word = sum(1 for sentence_count in sentence_word_count.values() if word in sentence_count)\n",
        "        idf[word] = math.log(total_sentences / (sentences_containing_word + 1))\n",
        "\n",
        "    return idf\n",
        "\n",
        "def output_idf(sentences, set_name):\n",
        "    idf = calculate_idf(sentences)\n",
        "\n",
        "    print(f\"{set_name}:\")\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        print(f\"|-------------------------IDF for Sentence {i}: -------------------------|\")\n",
        "        words = tokenize_sentence(sentence)\n",
        "        for word in words:\n",
        "            print(f\"|'{word}': {idf[word]}\")\n",
        "        print(f\"|----------------------------------------------------------------------|\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE362rdg0ZKE",
        "outputId": "384b172c-6836-495a-9eda-1644c7d9c903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences Set 1:\n",
            "|-------------------------IDF for Sentence 1: -------------------------|\n",
            "|'Python': -0.2876820724517809\n",
            "|'is': 0.4054651081081644\n",
            "|'a': 0.0\n",
            "|'great': 0.4054651081081644\n",
            "|'programming': 0.0\n",
            "|'language': 0.4054651081081644\n",
            "|----------------------------------------------------------------------|\n",
            "\n",
            "|-------------------------IDF for Sentence 2: -------------------------|\n",
            "|'Python': -0.2876820724517809\n",
            "|'can': 0.4054651081081644\n",
            "|'be': 0.4054651081081644\n",
            "|'used': 0.4054651081081644\n",
            "|'for': 0.4054651081081644\n",
            "|'a': 0.0\n",
            "|'wide': 0.4054651081081644\n",
            "|'variety': 0.4054651081081644\n",
            "|'of': 0.4054651081081644\n",
            "|'programming': 0.0\n",
            "|'tasks': 0.4054651081081644\n",
            "|----------------------------------------------------------------------|\n",
            "\n",
            "|-------------------------IDF for Sentence 3: -------------------------|\n",
            "|'It's': 0.4054651081081644\n",
            "|'easy': 0.4054651081081644\n",
            "|'to': 0.4054651081081644\n",
            "|'learn': 0.4054651081081644\n",
            "|'Python': -0.2876820724517809\n",
            "|----------------------------------------------------------------------|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print IDF for Sentences Set 1\n",
        "output_idf(sentences_1, \"Sentences Set 1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99VmhMdh0ZKF",
        "outputId": "0ccf569a-3b5e-4ab6-818c-a9cc79823a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences Set 2:\n",
            "|-------------------------IDF for Sentence 1: -------------------------|\n",
            "|'I': 0.4054651081081644\n",
            "|'love': 0.4054651081081644\n",
            "|'math': 0.4054651081081644\n",
            "|----------------------------------------------------------------------|\n",
            "\n",
            "|-------------------------IDF for Sentence 2: -------------------------|\n",
            "|'Math': 0.0\n",
            "|'is': 0.4054651081081644\n",
            "|'great': 0.4054651081081644\n",
            "|----------------------------------------------------------------------|\n",
            "\n",
            "|-------------------------IDF for Sentence 3: -------------------------|\n",
            "|'Math': 0.0\n",
            "|'rules': 0.4054651081081644\n",
            "|----------------------------------------------------------------------|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print IDF for Sentences Set 2\n",
        "output_idf(sentences_2, \"Sentences Set 2\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}